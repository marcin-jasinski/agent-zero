# Agent Zero (L.A.B.) - Implementation Plan

**Status**: Plan Approved ‚úì | Last Updated: 2026-01-28

**‚ö†Ô∏è IMPORTANT: Project Context**  
This is a **LOCAL DEVELOPMENT PLAYGROUND** for a single software engineer to experiment with AI agents and RAG pipelines on their local machine. This is **NOT** a production multi-user system. Design decisions and architecture prioritize simplicity, learnability, and experimentation over enterprise-scale concerns.

**üìã Code Review Status**: Comprehensive review completed (2026-01-28) - See [CODE_REVIEW_ADDENDUM.md](CODE_REVIEW_ADDENDUM.md) for findings contextualized to local dev usage.

---

## Table of Contents

1. [Project Definition](#project-definition)
2. [Implementation Phases](#implementation-phases)
3. [Code Review Findings & Improvements](#code-review-findings--improvements)
4. [Validation Checkpoints](#validation-checkpoints)
5. [Critical Design Decisions](#critical-design-decisions)
6. [Progress Tracking](#progress-tracking)

---

## Project Definition

### Branding & Concept

- **Name**: Agent Zero
- **Concept**: L.A.B. (Local Agent Builder)
- **Goal**: Production-ready, local-first environment for building and testing AI agents
- **UI Dashboard**: A.P.I. (AI Playground Interface)
- **Language**: STRICTLY ENGLISH (code, comments, UI, documentation)
- **Philosophy**: "One-Click Deployment", "Secure by Design", "Local-First"

### Technical Stack

- **Orchestration**: Docker Compose v2+
- **Language**: Python 3.11+
- **Core Libraries**: LangChain, Streamlit, Pydantic, LLM Guard, Pytest
- **Infrastructure Services**:
  - `app-agent`: Python 3.11 + Streamlit (port 8501)
  - `ollama`: LLM inference (port 11434, 8GB RAM limit)
  - `qdrant`: Vector Database (port 6333, 1GB RAM limit, persistent)
  - `meilisearch`: Full-text Search (port 7700, 1GB RAM limit)
  - `postgres`: Langfuse backing DB (port 5432, internal)
  - `langfuse`: Observability & Tracing UI (port 3000)

---

## Implementation Phases

### PHASE 1: Foundation & Infrastructure

**Goal**: Establish Docker environment and project skeleton with proper development setup.

#### Step 1: Project Structure & Git Setup

‚úÖ **COMPLETED** | Commit: 40e78e8

- [x] Create directory tree: `/src`, `/tests`, `/docker`, `/docs`, `/.devcontainer`
- [x] Initialize `pyproject.toml` with Python 3.11+ specification
- [x] Create `.env.example` with all required secrets/config
- [x] Initialize git branches: `master` (stable), `develop` (integration)
- [x] Extend `.gitignore` for Python/Docker artifacts

**Deliverable**: Clean project scaffold with no warnings from `git status` ‚úì

**Success Criteria**: All directories exist, `git status` shows only tracked files ‚úì

---

#### Step 2: Docker Compose Orchestration

‚úÖ **COMPLETED** | Commit: 294b879

- [x] Build `docker-compose.yml` with 6 services:
  - [x] `app-agent`: Python 3.11 + Streamlit (port 8501, 2GB RAM limit)
  - [x] `ollama`: LLM inference (port 11434, 8GB RAM limit)
  - [x] `qdrant`: Vector DB (port 6333, 1GB RAM limit, persistent volume)
  - [x] `meilisearch`: Full-text search (port 7700, 1GB RAM limit)
  - [x] `postgres`: Langfuse backing DB (port 5432, 512MB RAM limit, internal only)
  - [x] `langfuse`: Observability UI (port 3000, depends on postgres)
- [x] Define internal Docker DNS networking (services resolve via name)
- [x] Add health checks for critical services (ollama, qdrant, meilisearch, postgres)
- [x] Define named volumes for persistence (qdrant, postgres, meilisearch)
- [x] Add resource limits to all services

**Deliverable**: `docker-compose.yml` with resource limits enforced, passes `docker-compose config` validation ‚úì

**Success Criteria**: `docker-compose config` returns no errors, all services listed ‚úì

---

#### Step 3: DevContainer Configuration

‚úÖ **COMPLETED** | Commit: 1b4a8c9

- [x] Create `.devcontainer/devcontainer.json`:
  - [x] Uses `app-agent` service as development container
  - [x] Mounts workspace at `/app`
  - [x] Configures Python interpreter from container
- [x] Install VS Code extensions:
  - [x] Python (ms-python.python)
  - [x] Pylance (ms-python.vscode-pylance)
  - [x] Docker (ms-azuretools.vscode-docker)
  - [x] REST Client (humao.rest-client)
  - [x] GitLens, Black, Flake8, and 10+ more
- [x] Include initialization and customization scripts
- [x] Forward all service ports for development
- [x] Create Makefile with development commands
- [x] Add pre-commit hooks configuration

**Deliverable**: Dev experience functional: `F5` launches debugger, code changes reflected instantly ‚úì

**Success Criteria**: Open in DevContainer ‚Üí Debugger works ‚Üí Python interpreter detected ‚úì

---

#### Step 4: Configuration Management Layer

‚úÖ **COMPLETED** | Commit: 573483b

- [x] Create `src/config.py` using `pydantic-settings`:
  - [x] Define `DatabaseConfig` (Qdrant, Postgres)
  - [x] Define `OllamaConfig` (host, port, model)
  - [x] Define `MeilisearchConfig` (host, port)
  - [x] Define `LangfuseConfig` (host, port, API key)
  - [x] Include validation and defaults
- [x] Create `src/logging_config.py` for structured logging
- [x] Add environment-based validation (production enforces security)
- [x] Create test_config.py for verification

**Deliverable**: Single source of truth for all runtime configuration, type-safe ‚úì

**Success Criteria**: `from src.config import Config; Config()` succeeds and loads from `.env` ‚úì

---

#### Step 5: Repository Structure Validation

‚úÖ **COMPLETED** | Commit: a1b5054

- [x] Create stub `__init__.py` in all directories
- [x] Verify all paths referenced in Docker config exist
- [x] Create Streamlit UI stub (src/ui/main.py with 4 tabs)
- [x] Verify all critical Docker-referenced paths exist and are populated

**Deliverable**: Clean startup, all services healthy ‚úì

**Success Criteria**: All required files present, directory structure complete ‚úì

---

### PHASE 1 COMPLETE ‚úÖ

**Phase 1 Validation Status**:

- ‚úÖ `docker-compose.yml` with 6 services configured
- ‚úÖ All service resource limits enforced
- ‚úÖ Health checks for all critical services
- ‚úÖ Named volumes for persistence
- ‚úÖ DevContainer setup with VS Code integration
- ‚úÖ Configuration management with Pydantic
- ‚úÖ Logging system with JSON/text formatters
- ‚úÖ Makefile with development commands
- ‚úÖ Pre-commit hooks configuration
- ‚úÖ Repository structure complete

**Ready for Phase 2!** üöÄ

---

### PHASE 2: Core Application Skeleton

**Goal**: Build Streamlit UI (A.P.I.) with basic multi-tab navigation and service connectivity.

#### Step 6: Streamlit UI Frame (A.P.I.)

‚úÖ **COMPLETED** | Commit: `feature(ui): implement streamlit ui frame with chat, kb, settings, logs tabs`

- [x] Create `src/ui/main.py`:
  - [x] Configure Streamlit page (title, icon, layout)
  - [x] Implement 4 tabs: **Chat**, **Knowledge Base**, **Settings**, **Logs**
  - [x] Sidebar: Service health status indicators (integrated HealthChecker)
  - [x] Session state management for chat history
- [x] Create `src/ui/components/`:
  - [x] `chat.py` (message display, input form, export)
  - [x] `knowledge_base.py` (file upload widget, document list, search)
  - [x] `settings.py` (LLM parameters, database config, security settings)
  - [x] `logs.py` (real-time log viewer with filtering)
- [x] Implement error boundary UI for graceful failures

**Deliverable**: Streamlit app loads at `http://localhost:8501` with working HealthChecker integration

**Success Criteria**: ‚úì UI accessible, all tabs visible and responsive, ‚úì no Python errors on load, ‚úì sidebar shows real service health status

---

#### Step 7: Service Connectivity Layer

‚úÖ **COMPLETED** | Commit: `feature(services): implement service clients with 156 unit tests`

- [x] Create `src/services/__init__.py`
- [x] Implement service clients:
  - [x] `src/services/ollama_client.py` (pull models, generate embeddings, chat, health check)
  - [x] `src/services/qdrant_client.py` (create collections, upsert vectors, search, health check)
  - [x] `src/services/meilisearch_client.py` (create indexes, index documents, search, health check)
  - [x] `src/services/health_check.py` (HealthChecker class, ServiceStatus dataclass, aggregate health)
- [x] Wrap external SDKs with app-specific error handling and logging
- [x] Create comprehensive test suite: 156 unit tests across 4 test files
  - [x] `tests/services/test_ollama_client.py` (45 tests)
  - [x] `tests/services/test_qdrant_client.py` (39 tests)
  - [x] `tests/services/test_meilisearch_client.py` (43 tests)
  - [x] `tests/services/test_health_check.py` (29 tests)

**Deliverable**: Service clients testable in isolation (mock-friendly) with comprehensive edge-case coverage

**Success Criteria**: ‚úì All clients importable, ‚úì no external calls on import, ‚úì type hints complete, ‚úì 156 tests all passing, ‚úì code review completed

---

#### Step 8: Health Check & Startup Sequence

‚úÖ **COMPLETED** | Commit: `feature(startup): implement health check and startup sequence...`

- [x] Create `src/startup.py`:
  - [x] `ApplicationStartup` class with full initialization workflow
  - [x] `StartupStatus` dataclass for tracking step results
  - [x] `_check_services()`: Health check aggregation with graceful degradation
  - [x] `_initialize_ollama()`: Model availability and configuration
  - [x] `_initialize_qdrant()`: Collection creation with proper schema
  - [x] `_initialize_meilisearch()`: Index creation with searchable fields
  - [x] Comprehensive startup logging with visual status indicators
- [x] Integrate into `src/ui/main.py`:
  - [x] Run startup on first app load (one-time initialization)
  - [x] Store startup status in session state
  - [x] Graceful error handling for partial failures
- [x] Create comprehensive test suite: 20 unit tests
  - [x] `tests/test_startup.py` with full coverage
  - [x] Tests for each initialization step
  - [x] Tests for partial failures and error scenarios
  - [x] Tests for status retrieval and reporting

**Deliverable**: UI displays startup status and gracefully handles service unavailability

**Success Criteria**: ‚úì All services initialized on startup, ‚úì 20 tests passing, ‚úì Graceful degradation on partial failures, ‚úì Real-time status visible in logs

---

### PHASE 2 Progress Update

**Completed Items** ‚úÖ:

- ‚úÖ Step 6: Streamlit UI Frame with 4 components (690 lines)
- ‚úÖ Step 7: Service Connectivity with health checks (712 lines)
- ‚úÖ Step 8: Health Check & Startup Sequence (338 lines + 20 tests)
- ‚úÖ 196 comprehensive unit tests (Phase 2 total)
- ‚úÖ Code review completed, issues identified and fixed
- ‚úÖ Pre-commit checklist added to copilot-instructions.md

**Test Coverage Summary**:

- Service clients: 156 tests (Ollama 45, Qdrant 39, Meilisearch 43, HealthChecker 29)
- Startup sequence: 20 tests (status, initialization, full run, graceful degradation)
- **Total Phase 2: 196 tests**

### PHASE 2 Validation Checkpoint

**Phase 2 COMPLETE** ‚úÖ:

- [x] `docker-compose up -d` ‚Üí all services can start
- [x] Navigate to `http://localhost:8501` ‚Üí Streamlit UI loads
- [x] UI displays 4 tabs: Chat, Knowledge Base, Settings, Logs
- [x] Sidebar shows real health status from HealthChecker
- [x] Startup sequence runs on first application load
- [x] 196 unit tests passing, comprehensive edge-case coverage
- [x] Code review completed, all identified issues fixed
- [x] No Python errors on load, graceful error handling
- [x] DevContainer debugger ready for Phase 3

---

### PHASE 3: RAG Pipeline Integration

**Goal**: Implement document ingestion and retrieval augmented generation workflow.

#### Step 9: Document Ingestion Pipeline

‚úÖ **COMPLETED** | Commit: `feature(rag): implement document ingestion pipeline`

- [x] Create `src/core/ingest.py`:
  - [x] `ingest_pdf()`: Extract text from PDFs (using `pypdf`)
  - [x] `chunk_document()`: Split into overlapping chunks (500 tokens, 50 overlap)
  - [x] `generate_embeddings()`: Call Ollama for embeddings
  - [x] `store_in_qdrant()`: Upsert vectors with metadata
  - [x] `store_in_meilisearch()`: Index searchable text
- [x] Create `src/models/document.py`:
  - [x] `DocumentChunk` dataclass: `id`, `content`, `source`, `chunk_index`, `metadata`
  - [x] `DocumentMetadata` dataclass for document-level metadata
  - [x] `IngestionResult` dataclass for tracking ingestion status
- [x] Implement background job queue (thread pool for MVP)
- [x] Create comprehensive test suite: 20 unit tests

**Deliverable**: Upload PDF in UI ‚Üí document appears in Knowledge Base

**Success Criteria**: ‚úì PDF upload succeeds, ‚úì document metadata stored in both Qdrant and Meilisearch, ‚úì 20 tests passing

---

#### Step 10: Retrieval Engine

‚úÖ **COMPLETED** | Commit: `feature(rag): implement hybrid retrieval engine`

- [x] Create `src/core/retrieval.py`:
  - [x] `retrieve_relevant_docs()`: Query Qdrant by embedding similarity (top-k=5)
  - [x] Implement hybrid search: Qdrant (semantic) + Meilisearch (keyword)
  - [x] Return ranked list with scores
  - [x] `search_with_context()`: Retrieve surrounding chunks for better context
- [x] Create `src/models/retrieval.py`:
  - [x] `RetrievalResult` dataclass: `content`, `source`, `score`, `metadata`
  - [x] `HybridSearchConfig` dataclass for search configuration
- [x] Create comprehensive test suite: 18 unit tests

**Deliverable**: Search returns ranked results from both services

**Success Criteria**: ‚úì Query returns results with relevance scores, ‚úì results ranked correctly, ‚úì hybrid search combines semantic and keyword results

---

#### Step 11: Agent Orchestration with LangChain

‚úÖ **COMPLETED** | Commit: `feature(agent): implement agent orchestration with LangChain`

- [x] Create `src/core/agent.py`:
  - [x] Initialize agent with Ollama as LLM
  - [x] Define agent tools: `retrieve_documents`, `search_knowledge_base`, `get_current_time`
  - [x] Agent memory: `ConversationManager` (session-based multi-turn)
  - [x] Implement `process_message()` with streaming callback support
  - [x] Tool invocation with error handling
  - [x] Response generation with source attribution
- [x] Create `src/models/agent.py`:
  - [x] `AgentConfig` dataclass: `model_name`, `temperature`, `max_tokens`, `tools`
  - [x] `AgentMessage` dataclass for conversation messages
  - [x] `MessageRole` enum: user, assistant, system, tool
  - [x] `ConversationState` dataclass for session state
- [x] Create comprehensive test suite: 22 unit tests

**Deliverable**: Chat sends messages ‚Üí agent retrieves docs ‚Üí generates responses with sources

**Success Criteria**: ‚úì Agent responds in chat tab, ‚úì sources attributed in response, ‚úì tools called correctly

---

#### Step 12: Multi-Turn Conversation Memory

‚úÖ **COMPLETED** | Commit: `feature(agent): implement multi-turn conversation memory`

- [x] Implement `src/core/memory.py`:
  - [x] `ConversationManager`: Persist session history in-memory
  - [x] Methods: `add_message()`, `get_history()`, `clear_history()`, `delete_conversation()`
  - [x] Conversation summarization and context window management
  - [x] Multi-turn context retrieval with windowing
  - [x] Conversation metadata and tracking
- [x] Integrate into agent for automatic context management
- [x] Create comprehensive test suite: 30 unit tests

**Deliverable**: Chat maintains multi-turn context

**Success Criteria**: ‚úì Agent references previous messages correctly, ‚úì context window managed, ‚úì conversation state persisted

---

### PHASE 3 Progress Update

**Completed Items** ‚úÖ:

- ‚úÖ Step 9: Document Ingestion Pipeline (450+ lines)
- ‚úÖ Step 10: Retrieval Engine (320+ lines)
- ‚úÖ Step 11: Agent Orchestration (480+ lines)
- ‚úÖ Step 12: Multi-Turn Conversation Memory (280+ lines)
- ‚úÖ 92 comprehensive unit tests (Phase 3 total)
- ‚úÖ All code follows PEP 8 + type hints + Google-style docstrings
- ‚úÖ 100% mock-based testing (no external service calls)
- ‚úÖ Error handling and validation throughout

**Test Coverage Summary**:

- Document ingestion: 20 tests (chunking, extraction, embedding, storage)
- Retrieval engine: 18 tests (semantic, keyword, hybrid search)
- Agent orchestration: 22 tests (tool calling, response generation, memory)
- Conversation memory: 30 tests (session management, history, context)
- **Total Phase 3: 92 tests (all passing) ‚úÖ**

### PHASE 3 Validation Checkpoint

**Phase 3 COMPLETE** ‚úÖ:

- [x] Upload PDF via future Knowledge Base tab ‚Üí document is ingested
- [x] Document is chunked into overlapping segments
- [x] Embeddings generated and stored in Qdrant (semantic)
- [x] Document indexed in Meilisearch (full-text)
- [x] Search returns results from both semantic and keyword indices
- [x] Chat messages are added to conversation history
- [x] Agent retrieves relevant documents for user queries
- [x] Agent generates responses with source attribution
- [x] Multi-turn conversation maintains context window
- [x] 92 unit tests passing, all edge cases covered
- [x] Code review completed, all standards met
- [x] No Python errors on import, graceful error handling

**Next Steps**: Phase 4 - Security & Observability (LLM Guard, Langfuse)

---

### PHASE 3 Validation Checkpoint

**Phase 3 Complete When**:

- [ ] Upload PDF via "Knowledge Base" tab
- [ ] Document appears in document list
- [ ] Search in KB returns results with scores
- [ ] Type message in "Chat" tab
- [ ] Agent generates response with sources attributed
- [ ] Multi-turn conversation works (agent remembers previous messages)

---

### PHASE 4: Security & Observability

**Goal**: Add LLM Guard security and Langfuse tracing for production readiness.

#### Step 13: LLM Guard Integration

‚úÖ **COMPLETED** | Commit: `feature(security): implement LLM Guard integration`

- [x] Create `src/security/guard.py`:
  - [x] Input scanner: `scan_user_input()` ‚Üí detect prompt injection, toxic content
  - [x] Output scanner: `scan_llm_output()` ‚Üí detect harmful content, PII leakage
  - [x] Use `llm-guard` library with pre-built rules
- [x] Implement as middleware in agent execution
- [x] Log all blocked requests to observability layer
- [x] Create comprehensive test suite: 24 unit tests

**Deliverable**: Malicious inputs/outputs are sanitized or blocked ‚úì

**Success Criteria**: ‚úì Blocked content logged, ‚úì request rejected gracefully, ‚úì 24 tests passing, ‚úì integrated into agent

---

#### Step 14: Langfuse Observability

‚úÖ **COMPLETED** | Commit: `feature(observability): implement Langfuse observability integration`

- [x] **Re-enable Langfuse service** in `docker-compose.yml`:
  - [x] Uncommented the `langfuse` service definition (lines 250-295)
  - [x] Verified postgres and clickhouse dependencies are healthy
  - [x] Re-added `langfuse` to `app-agent` depends_on conditions
- [x] Create `src/observability/langfuse_callback.py`:
  - [x] Implemented LangfuseObservability class with singleton pattern
  - [x] Track: LLM calls, tool usage, agent decisions, execution time
  - [x] Include custom metrics: retrieval count, answer confidence
  - [x] Graceful degradation when Langfuse unavailable
- [x] Configured in agent initialization
- [x] Integrated with health check system
- [x] Create comprehensive test suite: 22 unit tests

**Deliverable**: Langfuse shows agent traces ‚úì

**Success Criteria**: ‚úì Langfuse UI accessible at `http://localhost:3000`, ‚úì Agent tracks all operations, ‚úì 22 tests passing, ‚úì Health check integration complete

---

#### Step 15: Environment-Specific Configuration

‚úÖ **COMPLETED** | Commit: `feature(config): add environment-specific configuration validation`

- [x] Implement `src/config.py` environment modes: `development`, `staging`, `production`
  - [x] Added `_validate_environment_requirements()` method with mode-specific enforcement
  - [x] Production: Strict validation (no debug, requires guards, no DEBUG logging)
  - [x] Staging: Guards required (LLM Guard + Langfuse), warns about debug
  - [x] Development: Flexible configuration with optional guards
- [x] Add `_log_environment_configuration()` for configuration visibility
- [x] Document nested configuration format (APP_SECURITY__<FIELD>) in docstring
- [x] Staging/production: Enable LLM Guard, require Langfuse ‚úì
- [x] Development: Optional guards, local tracing ‚úì
- [x] Add environment validation on startup ‚úì
- [x] Update `.env.example` with environment-specific documentation
- [x] Create comprehensive test suite: 38 unit tests (100% passing)

**Critical Discovery**: Nested Pydantic configs require `APP_<PARENT>__<FIELD>` format (e.g., `APP_SECURITY__LLM_GUARD_ENABLED`), not the nested config's `env_prefix`. Documented in config.py and .env.example.

**Deliverable**: Environment-based security enforcement ‚úì

**Success Criteria**: ‚úì Production enforces all security requirements, ‚úì Development allows flexible testing, ‚úì 38 tests passing, ‚úì Configuration errors provide actionable guidance

---

### PHASE 4b: Tool Dashboards & Management Interface

**Goal**: Transform Agent Zero from a chat-focused tool into a comprehensive management platform with vector database management, observability dashboards, and system monitoring.

**Prerequisites**: ‚úÖ Phase 4 Complete (Steps 13-15: LLM Guard, Langfuse Observability, Environment Configuration)

**Architecture**: Dynamic sidebar navigation with conditional tool rendering based on feature flags and service availability.

**Important**: **READ** [DASHBOARD_DESIGN.md](DASHBOARD_DESIGN.md) before implementation. It contains complete UI mockups, component specifications, data flow diagrams, and implementation guidelines.

---

#### Step 16: Dashboard Design & Architecture

‚úÖ **DESIGN DOCUMENT COMPLETE** | Reference: [DASHBOARD_DESIGN.md](DASHBOARD_DESIGN.md)

**Design Highlights**:
- **Sidebar Structure**: Core tools (Chat, KB, Settings, Logs) + Management tools (Qdrant, Langfuse, Promptfoo, System Health)
- **Component Architecture**: Modular tool components in `src/ui/tools/` with standardized interfaces
- **Data Flow**: Service clients ‚Üí Streamlit caching ‚Üí UI components ‚Üí User interactions
- **Performance**: `@st.cache_data` for all data fetching (5-minute TTL)
- **Error Handling**: Graceful degradation when services unavailable

**Key Implementation Patterns**:
```python
# Tool Interface Pattern
class ToolComponent:
    def render(self) -> None:
        """Render tool UI with error handling and caching."""
        
# Navigation Pattern
st.session_state["selected_tool"] = tool_name
# Conditional rendering based on selection
```

---

#### Step 17: Sidebar Navigation & Feature Flags ‚úÖ **COMPLETE**

**Completion Date**: 2026-01-28  
**Status**: ‚úÖ All tasks completed, 13/13 tests passing  
**Documentation**: See STEP_17_COMPLETION.md

**Implementation Order**: Foundation for all dashboard tools

- [x] **Add Feature Flags to Configuration** (`src/config.py`):
  - [x] Create `DashboardFeatures` nested config class:
    - [x] Core tools: `show_chat`, `show_knowledge_base`, `show_settings`, `show_logs` (default: `True`)
    - [x] Management tools: `show_qdrant_manager`, `show_langfuse_dashboard`, `show_promptfoo`, `show_system_health` (default: `False`)
  - [x] Add to `AppConfig` as `dashboard: DashboardFeatures`
  - [x] Environment variables: `APP_DASHBOARD__<FIELD>` naming convention
  - [x] Validation: warn if dashboard features enabled but services unavailable

- [x] **Create Navigation Component** (`src/ui/components/navigation.py`):
  - [x] `ToolDefinition` dataclass (key, icon, label, description, render_func, enabled, category)
  - [x] `SidebarNavigation` class:
    - [x] `register_tool()`: Add tools to navigation registry
    - [x] `get_enabled_tools()`: Filter by enabled status and category
    - [x] `render_sidebar()`: Render sidebar with tool buttons grouped by category
    - [x] `render_active_tool()`: Invoke active tool's render function with error handling
    - [x] `render()`: Main entry point - sidebar + content
    - [x] Track selection in `st.session_state["active_tool"]`
  - [x] Service health status indicator integrated in sidebar
  - [x] Visual separation between core and management tools

- [x] **Refactor Main UI** (`src/ui/main.py`):
  - [x] Replace tab-based navigation with sidebar navigation
  - [x] Extract existing components to tool modules:
    - [x] `src/ui/tools/chat.py` (moved from components/)
    - [x] `src/ui/tools/knowledge_base.py` (moved from components/)
    - [x] `src/ui/tools/settings.py` (moved from components/)
    - [x] `src/ui/tools/logs.py` (moved from components/)
  - [x] Create `setup_navigation()` function to register all tools
  - [x] Conditional tool rendering based on feature flags
  - [x] Maintain backward compatibility via component re-exports

- [x] **Create Test Suite** (`tests/ui/test_navigation.py`):
  - [x] Test tool loading with feature flags enabled/disabled
  - [x] Test ToolDefinition validation (empty key, non-callable render_func)
  - [x] Test navigation initialization and tool registration
  - [x] Test duplicate key prevention
  - [x] Test get enabled tools (all + category filtering)
  - [x] Test get active tool (found + not found)
  - [x] Test render active tool (success + no tool + error handling)
  - [x] 13 unit tests passing (100%)

**Deliverable**: ‚úÖ Dynamic sidebar navigation with feature flags, 13 tests passing, backward compatible

**Success Criteria**: ‚úì Feature flags control tool visibility, ‚úì sidebar navigation renders correctly, ‚úì tools grouped by category, ‚úì 13 tests passing, ‚úì backward compatibility maintained

---
  - [ ] Test tool selection and state management
  - [ ] Test sidebar rendering with different configurations
  - [ ] Test backward compatibility with existing tools
  - [ ] 8 unit tests minimum

**Design Reference**: DASHBOARD_DESIGN.md ¬ß "Sidebar Navigation Structure" (Lines 96-127)

**Deliverable**: Dynamic sidebar navigation with feature flag-controlled tool visibility

**Success Criteria**: ‚úì Navigation renders with icons, ‚úì tool selection switches content, ‚úì feature flags control management tool visibility, ‚úì all 4 existing tools work unchanged, ‚úì 8 tests passing

---

#### Step 18: Qdrant Manager Dashboard

‚úÖ **COMPLETED** | Commit: TBD | See: [STEP_18_COMPLETION.md](STEP_18_COMPLETION.md)

**Implementation Order**: After navigation (Step 17), first management tool

- [x] **Enhance Qdrant Client** (`src/services/qdrant_client.py`):
  - [x] Add `list_collections()`: Return list of all collections with metadata
  - [x] Add `get_collection_stats(collection_name: str)`: Return detailed stats
    - [x] Vector count, dimensions, distance metric, storage size, index config
  - [x] Add `search_by_text(query: str, collection: str, top_k: int)`:
    - [x] Convert query to embedding via Ollama
    - [x] Perform semantic search
    - [x] Return results with scores and payloads
  - [x] Add `create_collection_ui(name, vector_size, distance)`: Create collection with validation
  - [x] Add `delete_collection_ui(name)`: Delete with confirmation
  - [x] Error handling for all operations (collection not found, connection errors)

- [x] **Create Qdrant Dashboard Component** (`src/ui/tools/qdrant_dashboard.py`):
  - [x] **Collections Overview Section**:
    - [x] Display all collections in expandable cards
    - [x] Show: vector count, dimensions, storage size, distance metric
    - [x] Action buttons: [Details] [Search] [Delete]
    - [x] Create new collection form (name, vector size, distance metric dropdown)
    - [x] Refresh button with loading indicator
  - [x] **Search Interface Section**:
    - [x] Text input for semantic query
    - [x] Collection selector dropdown
    - [x] Top-K slider (1-20)
    - [x] Search button with loading state
    - [x] Results display: score, content preview, metadata
    - [x] Expandable result details (color-coded by score: üü¢ ‚â•0.8, üü° ‚â•0.6, üî¥ <0.6)
  - [x] **Collection Details in Expandable Cards**:
    - [x] Full configuration view
    - [x] Vector distribution statistics
    - [x] Index performance metrics
  - [x] Use `@st.cache_data(ttl=300)` for collections list
  - [x] Use `@st.cache_data(ttl=60)` for search results
  - [x] Error handling with user-friendly messages

- [x] **Create Test Suite** (`tests/services/test_qdrant_client.py`):
  - [x] Test `list_collections()` with empty/multiple collections (4 tests)
  - [x] Test `get_collection_stats()` with valid/invalid collection (3 tests)
  - [x] Test `search_by_text()` with various queries (5 tests)
  - [x] Test collection creation with validation (6 tests)
  - [x] Test collection deletion with confirmation (4 tests)
  - [x] Test error scenarios (Qdrant unavailable, invalid params)
  - [x] Mock Ollama embedding generation
  - [x] **23 unit tests added** (exceeds 16 minimum) - **All passing ‚úÖ**

- [x] **Navigation Integration** (`src/ui/main.py`, `src/ui/tools/__init__.py`):
  - [x] Import and register `render_qdrant_dashboard` in navigation
  - [x] Feature flag: `APP_DASHBOARD__SHOW_QDRANT_MANAGER=true`

**Design Reference**: DASHBOARD_DESIGN.md ¬ß "Qdrant Manager Tab" (Lines 163-189) ‚úì

**Implementation Summary**:
- **QdrantClient:** +200 lines (5 new methods with validation, error handling, logging)
- **Dashboard:** +372 lines (complete UI component with caching, color-coding, confirmations)
- **Tests:** +280 lines (23 comprehensive tests, 49 total Qdrant tests)
- **Total:** +862 lines, 5 files changed

**UI Layout**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ üîç Qdrant Manager               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Collections Overview:           ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇ üìÅ documents                ‚îÇ ‚îÇ
‚îÇ ‚îÇ Vectors: 8,432  Size: 768   ‚îÇ ‚îÇ
‚îÇ ‚îÇ Storage: 12.5 MB            ‚îÇ ‚îÇ
‚îÇ ‚îÇ [Details] [Search] [Delete] ‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                 ‚îÇ
‚îÇ Search Interface:               ‚îÇ
‚îÇ Query: [____________] [Search]  ‚îÇ
‚îÇ Collection: [documents ‚ñº]       ‚îÇ
‚îÇ Top K: [5 ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ] (1-20)      ‚îÇ
‚îÇ                                 ‚îÇ
‚îÇ Results: 5 matches              ‚îÇ
‚îÇ 1. üü¢ 0.92 | "text..."         ‚îÇ
‚îÇ 2. üü° 0.87 | "text..."         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Deliverable**: Qdrant Manager dashboard with collections management and semantic search ‚úÖ

**Success Criteria**: 
- ‚úÖ View all collections with stats
- ‚úÖ Semantic search by text query with color-coded results
- ‚úÖ Create/delete collections with validation and confirmation
- ‚úÖ Responsive UI with caching (5min for collections, 1min for search)
- ‚úÖ 23 tests passing (exceeds 16 minimum)
- ‚úÖ Feature flag integration (opt-in)
- ‚úÖ Design specification compliance

---

#### Step 19: Langfuse Observability Dashboard

‚úÖ **COMPLETED** | Implementation Date: Phase 4b Completion

**Implementation Order**: After Qdrant dashboard (Step 18)

- [x] **Create Langfuse Client** (`src/services/langfuse_client.py` - NEW):
  - [x] Read-only HTTP API wrapper for Langfuse
  - [x] Configuration from `LangfuseConfig` (host, public_key, secret_key)
  - [x] Methods:
    - [x] `get_trace_summary(time_range: str)`: Total traces, avg latency, error rate
    - [x] `get_recent_traces(limit: int, filter: dict)`: Recent traces with metadata
    - [x] `get_trace_details(trace_id: str)`: Full trace with spans and events
    - [x] `is_healthy()`: Check Langfuse API connectivity
  - [x] Use requests library with retry logic and timeouts
  - [x] Handle authentication (API keys)
  - [x] Graceful error handling (service unavailable)
  - [x] Response parsing and validation

- [x] **Create Langfuse Dashboard Component** (`src/ui/tools/langfuse_dashboard.py`):
  - [x] **Summary Metrics Section**:
    - [x] Display: total traces, traces last 24h, avg latency, error rate
    - [x] Use st.metrics() for visual cards
    - [x] Time range selector: 24h, 7d, 30d
    - [x] Refresh button with last updated timestamp
  - [x] **Recent Traces Section**:
    - [x] Scrollable list of recent traces (limit 20)
    - [x] Display: timestamp, trace name, duration, status (‚úì/‚úó)
    - [x] Token counts (input/output) if available
    - [x] Expandable trace details
    - [x] Filter options: status (all/success/error), time range
  - [x] **Trace Details Viewer** (expandable):
    - [x] Full trace hierarchy (spans)
    - [x] Token usage breakdown
    - [x] Latency breakdown by component
    - [x] Error messages if present
  - [x] **Link to Full Langfuse UI**:
    - [x] Button to open full dashboard (port 3000)
    - [x] Display current Langfuse connection status
  - [x] Use `@st.cache_data(ttl=60)` for trace summary
  - [x] Use `@st.cache_data(ttl=30)` for recent traces
  - [x] Handle Langfuse disabled state gracefully

- [x] **Create Test Suite** (`tests/services/test_langfuse_client.py` + `tests/ui/tools/test_langfuse_dashboard.py`):
  - [x] Test `get_trace_summary()` with various time ranges
  - [x] Test `get_recent_traces()` with filtering
  - [x] Test `get_trace_details()` with valid/invalid trace IDs
  - [x] Test authentication handling
  - [x] Test error scenarios (Langfuse unavailable, invalid credentials)
  - [x] Test UI rendering with empty/populated data
  - [x] Test time range filtering
  - [x] Mock HTTP API responses
  - [x] 24 unit tests implemented (exceeds 14 minimum)

**Implementation Files Created**:
- `src/services/langfuse_client.py` (464 lines) - Complete HTTP API client
- `src/ui/tools/langfuse_dashboard.py` (~350 lines) - Full dashboard component
- `tests/services/test_langfuse_client.py` (24 tests) - Comprehensive test coverage

**Deliverable**: Langfuse observability dashboard with trace viewing and metrics ‚úì

**Success Criteria**: ‚úì Display trace summary, ‚úì show recent traces, ‚úì filter by time range, ‚úì view trace details, ‚úì link to full Langfuse UI, ‚úì 24 tests passing

---

#### Step 20: Implement Promptfoo Testing Dashboard (Optional)

- [ ] Create `src/ui/tools/promptfoo_dashboard.py`:
  - [ ] Test scenario creation interface
  - [ ] Test run history with version management
  - [ ] Results comparison between versions
  - [ ] Pass/fail rate visualization
  - [ ] Deploy selected prompt version
- [ ] Create test suite: 12 unit tests
  - [ ] Test creation
  - [ ] Test run management
  - [ ] Version comparison
  - [ ] Error handling

**Note**: Implementation depends on Promptfoo library availability. Can be mocked for MVP.

**Deliverable**: Promptfoo tab for prompt testing and versioning

**Success Criteria**: ‚úì Create test scenarios, ‚úì view test results, ‚úì compare versions, ‚úì 12 tests passing

---

#### Step 21: System Health Dashboard

‚úÖ **COMPLETED** | Implementation Date: Phase 4b Completion

**Implementation Order**: After Langfuse dashboard (Step 19) or Promptfoo (Step 20)

- [x] **Enhance Health Check Service** (`src/services/health_check.py`):
  - [x] Add `get_detailed_status()`: Per-service detailed metrics
    - [x] Response times, last check timestamp, error messages
    - [x] Service versions if available (Ollama version, Qdrant version)
  - [x] Add `get_resource_metrics()`: Host system resource usage
    - [x] CPU usage, memory usage, disk space
    - [x] Docker container stats (if available)
  - [x] Add `get_historical_data(time_range: str)`: Status over time
    - [x] Uptime percentage per service
    - [x] Response time trends
  - [x] Add `restart_service(service_name: str)`: Trigger restart (via Docker API if available)
  - [x] Caching with short TTL (30 seconds)

- [x] **Create System Health Dashboard Component** (`src/ui/tools/system_health.py`):
  - [x] **Service Status Overview Section**:
    - [x] Status cards for each service (Ollama, Qdrant, Meilisearch, Langfuse)
    - [x] Color-coded indicators: üü¢ Healthy, üü° Degraded, üî¥ Down
    - [x] Display: status, response time, uptime, version
    - [x] Refresh button with auto-refresh toggle (30s interval)
  - [x] **Host Resources Section**:
    - [x] CPU usage meter (with visual gauge)
    - [x] Memory usage meter (with visual gauge)
    - [x] Disk space meter (with visual gauge)
    - [x] Docker container resource usage (if available)
  - [x] **Historical Trends Section** (Optional):
    - [x] Line charts for response times over last 24h
    - [x] Uptime percentage bars per service
    - [x] Incident log (recent failures)
  - [x] **Actions Section**:
    - [x] Restart service buttons (with confirmation)
    - [x] Run full health check button
    - [x] Export diagnostics button (JSON download)
  - [x] Use `@st.cache_data(ttl=30)` for health status
  - [x] Auto-refresh option with `st.rerun()` every 30s

- [x] **Create Test Suite** (`tests/services/test_health_check.py` + `tests/ui/tools/test_system_health.py`):
  - [x] Test `get_detailed_status()` with all services
  - [x] Test `get_resource_metrics()` with various resource levels
  - [x] Test `get_historical_data()` with different time ranges
  - [x] Test service restart logic (mocked)
  - [x] Test UI rendering with healthy/degraded/down states
  - [x] Test auto-refresh behavior
  - [x] Test resource gauges rendering
  - [x] Mock psutil and Docker API calls
  - [x] 15 unit tests implemented

**Implementation Files Created**:
- `src/ui/tools/system_health.py` (~404 lines) - Complete health dashboard
- `tests/ui/tools/test_system_health.py` (15 tests) - Test coverage

**Deliverable**: System Health dashboard with real-time service monitoring ‚úì

**Success Criteria**: ‚úì Display all service statuses, ‚úì show host resources, ‚úì auto-refresh functionality, ‚úì restart services (if Docker API available), ‚úì 15 tests passing

---

#### Step 22: Integration Testing & Validation

‚úÖ **COMPLETED** | Implementation Date: Phase 4b Completion

**Implementation Order**: Final step after all dashboard tools implemented (Steps 17-21)

- [x] **Create Integration Test Suite** (`tests/integration/test_dashboard_integration.py`):
  - [x] Test sidebar navigation between all tools
  - [x] Test feature flag toggling (enable/disable tools dynamically)
  - [x] Test data flow: User interaction ‚Üí Service client ‚Üí Backend
  - [x] Test caching behavior across page refreshes
  - [x] Test error handling propagation (service unavailable scenarios)
  - [x] Test concurrent tool usage (multiple tabs/sessions)
  - [x] Test backward compatibility (existing 4 tools still work)
  - [x] 23 integration tests implemented (exceeds 10 minimum)

- [x] **Performance Testing**:
  - [x] Measure page load times for each dashboard tool
  - [x] Benchmark: <500ms initial load, <200ms cached load
  - [x] Test with large datasets (1000+ vectors, 100+ traces)
  - [x] Profile Streamlit caching effectiveness
  - [x] Identify and fix performance bottlenecks

**Implementation Files Created**:
- `tests/integration/test_dashboard_integration.py` (490 lines, 23 tests)
- Enhanced `tests/conftest.py` with comprehensive fixtures

**Test Suite Summary**:
- TestSidebarNavigation: 4 tests
- TestFeatureFlagToggling: 2 tests
- TestDataFlow: 3 tests
- TestCachingBehavior: 2 tests
- TestErrorHandling: 2 tests
- TestBackwardCompatibility: 3 tests
- TestToolDefinitionValidation: 3 tests
- TestEnvironmentIntegration: 2 tests
- TestDashboardUIConsistency: 2 tests

**Deliverable**: Complete integration test suite ‚úì

**Success Criteria**: ‚úì All integration tests pass (23 tests), ‚úì performance benchmarks met, ‚úì backward compatibility verified

- [x] **End-to-End Workflow Validation**:
  - [ ] **Scenario 1: Knowledge Base Management**:
    - [ ] Upload document via Knowledge Base tool
    - [ ] Verify in Qdrant Manager (collection updated)
    - [ ] Search via Qdrant dashboard
    - [ ] Query via Chat tool
    - [ ] View trace in Langfuse dashboard
  - [ ] **Scenario 2: System Monitoring**:
    - [ ] Generate 50 chat queries
    - [ ] Monitor service health in real-time
    - [ ] View traces in Langfuse
    - [ ] Check resource usage trends
  - [ ] **Scenario 3: Feature Flag Testing**:
    - [ ] Disable Langfuse via feature flag
    - [ ] Verify Langfuse tab hidden
    - [ ] Verify agent still works without observability
    - [ ] Re-enable and verify tab reappears

- [ ] **User Acceptance Testing Checklist**:
  - [ ] All 6 tabs render correctly
  - [ ] Navigation smooth with no errors
  - [ ] Caching reduces redundant API calls
  - [ ] Error messages user-friendly
  - [ ] Mobile responsive (basic)
  - [ ] Documentation updated

- [ ] **Validation Checkpoints**:
  - [ ] Verify 60+ tests passing (Steps 17-21 combined)
  - [ ] No regressions in existing functionality
  - [ ] All services accessible via Docker Compose
  - [ ] Configuration via .env working
  - [ ] Logs clear and actionable

**Design Reference**: DASHBOARD_DESIGN.md ¬ß "Data Flow & Caching" (Lines 402-442)

**Deliverable**: Complete Phase 4b with validated, production-ready dashboard

**Success Criteria**: ‚úì All integration tests pass, ‚úì performance benchmarks met, ‚úì E2E workflows validated, ‚úì no regressions, ‚úì documentation complete

---

### PHASE 4b Progress Tracking

**Estimated Effort**: 3-4 weeks (following DASHBOARD_DESIGN.md)

**Components**: 5 dashboard tools + 1 navigation component + 3 service client enhancements

**Test Coverage**: 60+ unit tests (Navigation 8, Qdrant 16, Langfuse 14, Promptfoo 12, Health 18, Integration 10)

---

### PHASE 4b Validation Checkpoint

**Phase 4b Complete When**:

- [x] Sidebar navigation renders with all tools
- [x] Feature flags control tool visibility dynamically
- [x] Qdrant Manager tab shows collections and search
- [x] Langfuse Observability tab shows recent traces
- [x] System Health tab shows all service metrics
- [x] Promptfoo tab works (if implemented) or gracefully skipped
- [x] All 60+ tests passing (98+ achieved)
- [x] Dashboard components responsive and cached
- [x] No external calls without error handling
- [x] Backward compatibility with existing 4 tools verified

‚úÖ **PHASE 4b VALIDATED** | 399 tests passing, 5 skipped

---

### PHASE 4 Validation Checkpoint

**Phase 4 Complete When**:

- [ ] Malicious input test ‚Üí blocked and logged
- [ ] LLM Guard metrics visible in observability
- [ ] Langfuse dashboard accessible and shows traces
- [ ] Environment modes work correctly
- [ ] Production mode enforces security requirements

---

### PHASE 5: Testing, Documentation & UX Polish

**Goal**: Ensure code quality, developer experience, and address code review findings.

**Updated Based on Code Review (2026-01-28)**: Added Steps 19.5-19.8 for UX improvements and documentation polish.

#### Step 16: Pytest Setup & Fixtures

- [ ] Create `tests/conftest.py`:
  - [ ] Mock Ollama client (return dummy embeddings)
  - [ ] Mock Qdrant client (in-memory storage)
  - [ ] Mock Meilisearch client (return fake results)
  - [ ] Mock Langfuse callback
- [ ] Create `tests/fixtures/`:
  - [ ] Sample PDFs, embeddings, Qdrant responses
- [ ] Implement `pytest.ini` with coverage thresholds (80% core logic)

**Deliverable**: Fast test execution without external services

**Success Criteria**: `pytest` runs in <5 seconds, 100% mocked external services

---

#### Step 17: Unit Tests for Core Modules

- [ ] `tests/core/test_ingest.py`: Verify chunking, embedding generation
- [ ] `tests/core/test_retrieval.py`: Verify ranking, hybrid search
- [ ] `tests/core/test_agent.py`: Verify tool calling, memory management
- [ ] `tests/security/test_guard.py`: Verify input/output scanning
- [ ] Each test: arrange ‚Üí act ‚Üí assert, no external calls

**Deliverable**: Unit tests with >80% coverage on core modules

**Success Criteria**: `pytest --cov=src core logic` ‚â•80%, all tests pass

---

#### Step 18: Integration Tests (Docker-dependent)

- [ ] Create `tests/integration/test_e2e_chat.py`:
  - [ ] Mark with `@pytest.mark.integration`
  - [ ] Spin up services, upload document, run chat, verify end-to-end
  - [ ] Include in CI/CD, skip locally by default
- [ ] Documentation on running: `pytest -m integration`

**Deliverable**: Integration tests pass against real Docker services

**Success Criteria**: `pytest -m integration` passes, e2e workflow verified

---

#### Step 19: Documentation & README

- [ ] Create `README.md`:
  - [ ] Badges: License, Python version, Docker, Build status
  - [ ] "What is Agent Zero?" section
  - [ ] "Quick Start" (3 steps: clone, `docker-compose up`, open `http://localhost:8501`)
  - [ ] Architecture diagram (Mermaid): UI ‚Üí Services ‚Üí LLMs ‚Üí DBs
  - [ ] Development guide: Running tests, debugging, adding custom tools
  - [ ] API docs for service clients
  - [ ] Troubleshooting section
- [ ] Create `ARCHITECTURE.md`: Detailed layer diagrams, data flow
- [ ] Create `CONTRIBUTING.md`: Commit conventions (Conventional Commits), branch naming

**Deliverable**: New developer can run project and understand architecture in <20 minutes

**Success Criteria**: README is clear, architecture diagram is accurate, Quick Start works

---

#### Step 19.5: UX Improvements (Code Review Priority üî¥)

‚úÖ **COMPLETED** | Date: 2026-01-30

**Added from Code Review**: Address high-priority user experience issues

- [x] **Add Progress Indicators**:
  - [x] Chat tab: Show spinner during LLM calls with "Agent is thinking..."
  - [x] Knowledge Base: Progress bar for document ingestion (multi-step)
  - [x] All service calls: Spinner with status indication
  - [x] Success/error feedback after operations (balloons on success)
- [x] **Surface Errors to UI**:
  - [x] Replace silent failures with `st.error()` messages
  - [x] Add troubleshooting hints for common errors
  - [x] Show actionable next steps ("Check if Ollama is running")
  - [x] Log errors with full context (stack traces)
  - [x] Added retry buttons for connection errors
- [x] **Complete Knowledge Base Features**:
  - [x] Document upload with multi-step progress
  - [x] Document search with progress indication
  - [x] Document list with metadata
  - [x] Delete document capability
- [x] **Improve Startup Experience**:
  - [x] Add retry logic with exponential backoff for services
  - [x] Better error messages if services not ready
  - [x] Show which service is causing startup failure
  - [x] Added troubleshooting commands in logs

**Deliverable**: UI provides clear feedback during all operations ‚úì

**Success Criteria**: ‚úì All operations show progress, ‚úì Errors visible to user, ‚úì Knowledge Base fully functional

---

#### Step 19.6: Example Content & Onboarding

‚úÖ **COMPLETED** | Date: 2026-01-29

**Added from Code Review**: Improve learning experience with examples

- [x] **Sample Documents**:
  - [x] Add 2-3 sample Markdown docs to `data/samples/`
  - [x] Topics: RAG explanation, embeddings guide, Agent architecture
  - [x] Created: `rag_introduction.md`, `embeddings_guide.md`, `agent_architecture.md`
- [x] **Example Queries**:
  - [x] Add suggested queries in Chat tab
  - [x] "Try asking: What is RAG? How do embeddings work?"
  - [x] Shows 4 example query buttons for new users
- [ ] **Quick Start Tutorial** (deferred):
  - [ ] Add help button in UI
  - [ ] Show guided tour on first launch

**Deliverable**: New users have immediate hands-on examples ‚úì

**Success Criteria**: ‚úì Sample documents available, ‚úì Example queries provided

---

#### Step 19.7: Documentation Polish (Code Review Priority üü°)

‚úÖ **COMPLETED** | Date: 2026-01-29

**Added from Code Review**: Clarify local dev context and security

- [x] **Update README.md**:
  - [x] Add prominent "‚ö†Ô∏è FOR LOCAL DEVELOPMENT ONLY" section
  - [x] Warn: "Do NOT expose port 8501 to public internet"
  - [x] Explain default passwords are for localhost only
  - [x] Add section: "What This Is (And Isn't)"
  - [x] Clarify single-user experimentation focus
- [x] **Docker Compose Comments**:
  - [x] Add security warning header to docker-compose.yml
  - [x] Add "‚ö†Ô∏è LOCAL DEV ONLY" to password defaults
  - [x] Document why defaults are okay for localhost
- [x] **Troubleshooting Guide**:
  - [x] "Service not starting" diagnostics
  - [x] "Ollama model not found" solutions
  - [x] "Out of memory" guidance
  - [x] Port conflicts resolution
  - [x] Search not working solutions

**Deliverable**: Clear documentation of local dev nature and security context ‚úì

**Success Criteria**: ‚úì Local dev context clear in README, ‚úì Security warnings present, ‚úì Troubleshooting guide complete

---

#### Step 19.8: Testing Improvements (Code Review Priority üü°)

‚úÖ **COMPLETED** | Date: 2026-01-29

**Added from Code Review**: Address test coverage gaps

- [x] **Enable Skipped Tests**:
  - [x] Fix security tests in `tests/security/test_guard.py`
  - [x] Remove `@pytest.mark.skip` decorators (5 tests)
  - [x] Mock llm-guard scanners properly for testing
  - [x] Tests now validate: prompt injection, toxicity, multiple violations, malicious URLs, sensitive data
- [x] **Integration Tests**:
  - [x] Dashboard integration tests in `tests/ui/test_dashboard_integration.py`
  - [x] 23 integration tests for UI workflows
- [x] **Test Error Scenarios**:
  - [x] Service unavailable handling (tested across all service clients)
  - [x] Timeout scenarios (health check tests)
  - [x] Invalid input handling (validation tests)
  - [x] Malformed responses (error boundary tests)

**Deliverable**: Comprehensive test coverage including error paths ‚úì

**Success Criteria**: ‚úì 404 tests passing, ‚úì 0 skipped, ‚úì Security tests enabled

**Success Criteria**: ‚úì No skipped tests, ‚úì Integration tests passing, ‚úì Error scenarios covered

---

### PHASE 5 Validation Checkpoint

**Phase 5 Complete When**:

**Original Goals**:
- [ ] `pytest` runs with >80% coverage
- [ ] `pytest -m integration` passes
- [ ] README is complete and clear
- [ ] Architecture diagram is accurate
- [ ] New developer can understand project in <20 minutes

**Added from Code Review**:
- [ ] All operations show progress indicators in UI
- [ ] Errors are visible to user with troubleshooting hints
- [ ] Knowledge Base upload/search fully implemented
- [ ] Startup errors are clear and actionable
- [ ] "LOCAL DEV ONLY" warnings present in documentation
- [ ] Example documents and queries available
- [ ] Security tests enabled and passing
- [ ] At least one e2e integration test passing
- [ ] Troubleshooting guide complete

---

## Validation Checkpoints

### Summary Table

| Phase        | Checkpoint                                                           | Status         | Notes                                          |
| ------------ | -------------------------------------------------------------------- | -------------- | ---------------------------------------------- |
| **Phase 1**  | `docker-compose up -d` ‚Üí all services healthy, DevContainer works    | ‚úÖ COMPLETED   | Foundation ready for Phase 2                   |
| **Phase 2**  | UI loads, services connect, health checks display correctly          | ‚úÖ COMPLETED   | Core UI & connectivity ready for Phase 3       |
| **Phase 3**  | Upload PDF ‚Üí search in KB ‚Üí chat generates responses with sources    | ‚úÖ COMPLETED   | RAG pipeline ready for Phase 4                 |
| **Phase 4**  | Malicious input blocked, Langfuse shows traces                       | ‚è≥ Not Started | Security & observability                       |
| **Phase 4b** | All dashboard tools accessible, metrics displayed, 60+ tests passing | ‚è≥ Not Started | Management interface (see DASHBOARD_DESIGN.md) |
| **Phase 5**  | `pytest --cov‚â•80%`, README clear to new developer                    | ‚è≥ Not Started | Testing & docs                                 |

---

## Critical Design Decisions

### 1. Single Service Architecture

- **Decision**: All logic in `app-agent` container (no separate API gateway)
- **Rationale**: Simplifies deployment, reduces complexity for "One-Click Deployment"
- **Trade-off**: Vertical scaling limited; horizontal scaling requires stateless redesign later

### 2. Synchronous Execution

- **Decision**: No message queues initially; async jobs handled via threading
- **Rationale**: MVP simplicity; scales for single-user development
- **Migration Path**: Upgrade to Celery + Redis for multi-user production

### 3. Session-Based Memory

- **Decision**: In-memory conversation store per session
- **Rationale**: Fast, no schema design needed; suitable for single-user
- **Upgrade**: Migrate to SQLite (Phase 2b), then Postgres for multi-user

### 4. Dual Indexing (Meilisearch + Qdrant)

- **Decision**: Index all documents in both Meilisearch (keyword) and Qdrant (semantic)
- **Rationale**: Ensures both semantic AND keyword search work, improves recall
- **Cost**: Higher storage, but acceptable for local deployment

### 5. Langfuse Tracing for All Operations

- **Decision**: Every agent decision, tool call, and LLM interaction logged
- **Rationale**: Explainability and debugging; critical for "Secure by Design"
- **Performance**: Minimal overhead; local Langfuse instance

### 6. Environment-Specific Security

- **Decision**: `development` mode is permissive; `production` enforces all guards
- **Rationale**: Fast iteration during development; secure defaults in production
- **Implementation**: Startup validation checks environment and adjusts settings

---

## Code Review Findings & Improvements

**Review Date**: 2026-01-28  
**Context**: Local single-user development playground  
**Full Reports**: [CODE_REVIEW_ADDENDUM.md](CODE_REVIEW_ADDENDUM.md), [CRITICAL_CODE_REVIEW.md](CRITICAL_CODE_REVIEW.md)

**Overall Assessment**: **Grade B+** - Well-designed for local experimentation with targeted improvements needed

### Phase 1-3: Existing Implementation Review

#### ‚úÖ What's Working Well (Keep As-Is)

1. **Architecture for Single-User Local Dev**
   - ‚úÖ Synchronous I/O is appropriate (no concurrent users)
   - ‚úÖ In-memory conversation storage acceptable for experimentation
   - ‚úÖ Streamlit session state usage is correct pattern
   - ‚úÖ Default passwords in docker-compose OK for localhost (with documentation)
   - ‚úÖ No authentication needed for localhost-only usage
   - ‚úÖ Current service client approach works fine for local scale

2. **Code Quality**
   - ‚úÖ Clean project structure with clear separation of concerns
   - ‚úÖ Comprehensive type hints throughout
   - ‚úÖ Good use of Pydantic for configuration
   - ‚úÖ Google-style docstrings present
   - ‚úÖ Test coverage for core functionality (312 tests)

3. **Development Experience**
   - ‚úÖ Easy docker-compose setup
   - ‚úÖ DevContainer integration working
   - ‚úÖ Hot reload for fast iteration
   - ‚úÖ Clear code structure for learning

#### üî¥ High Priority Issues (Affects User Experience)

**MUST FIX for Phase 5:**

1. **Missing Progress Indicators** - HIGH IMPACT
   - **Issue**: UI freezes during long operations (LLM calls 10-30s, document ingestion 30-60s) with no feedback
   - **Impact**: User doesn't know if app is working or crashed
   - **Files Affected**: `src/ui/components/chat.py`, `src/ui/components/knowledge_base.py`
   - **Fix**: Add progress spinners, status messages, and estimated time
   - **Example**:
     ```python
     # Current (no feedback during LLM call)
     response = agent.process_message(message)
     
     # Should be (with progress)
     with st.spinner("ü§î Agent is thinking..."):
         response = agent.process_message(message)
     st.success("‚úÖ Response generated!")
     ```
   - **Priority**: üî¥ HIGH - Directly affects experimentation experience

2. **Poor Error Visibility** - HIGH IMPACT
   - **Issue**: Errors logged but not shown to user; silent failures confuse during experimentation
   - **Impact**: User doesn't know what went wrong or how to fix it
   - **Files Affected**: Throughout `src/services/`, `src/core/`
   - **Fix**: Surface errors in UI with actionable messages
   - **Example**:
     ```python
     # Current (silent failure)
     except Exception as e:
         logger.error(f"Failed: {e}")
         return []
     
     # Should be (user-visible)
     except Exception as e:
         logger.error(f"Failed: {e}", exc_info=True)
         st.error(f"‚ùå Operation failed: {str(e)}")
         st.info("üí° Troubleshooting: Check if Ollama service is running")
         return []
     ```
   - **Priority**: üî¥ HIGH - Critical for debugging during learning

3. **Incomplete Features (TODOs)** - HIGH IMPACT
   - **Issue**: Knowledge Base has TODOs instead of working upload/search
   - **Files**: `src/ui/components/knowledge_base.py:68,127`
   - **Impact**: Advertised features don't work; confusing for users
   - **Fix**: Either implement or remove features
   - **Priority**: üî¥ HIGH - Affects core RAG experimentation

4. **Startup Error Messages** - HIGH IMPACT
   - **Issue**: Unclear errors if services aren't ready; no retry logic
   - **Files**: `src/startup.py`
   - **Impact**: First-time users get confused by cryptic errors
   - **Fix**: Better error messages, automatic retries, clear instructions
   - **Priority**: üî¥ HIGH - Affects first impression

#### üü° Medium Priority Issues (Should Fix for Quality)

**RECOMMENDED for Phase 5:**

1. **Security Documentation Gaps**
   - **Issue**: Default passwords need "FOR LOCAL DEV ONLY" warnings
   - **Files**: `docker-compose.yml`, `README.md`
   - **Fix**: Add prominent warnings and guidance for network exposure
   - **Priority**: üü° MEDIUM

2. **ClickHouse Password Hardcoded**
   - **Issue**: No env var override option (line 222)
   - **Files**: `docker-compose.yml`
   - **Fix**: Make it overridable like other services
   - **Priority**: üü° MEDIUM

3. **Test Coverage Gaps**
   - **Issue**: Security tests skipped, no integration tests
   - **Files**: `tests/security/test_guard.py`, `tests/integration/`
   - **Fix**: Enable skipped tests, add basic integration tests
   - **Priority**: üü° MEDIUM

4. **Better Logging for Learning**
   - **Issue**: Logs don't show RAG pipeline details
   - **Fix**: Log retrieval results, similarity scores, agent decisions
   - **Priority**: üü° MEDIUM - Helps understand how RAG works

5. **Code Quality Issues**
   - **Issue**: Broad exception handling, inconsistent error return types
   - **Files**: Throughout codebase
   - **Fix**: More specific exception types, consistent error handling
   - **Priority**: üü° MEDIUM

#### üü¢ Low Priority Improvements (Nice-to-Have)

**OPTIONAL for Future Phases:**

1. **Export/Import Conversations**
   - **Feature**: Save interesting sessions for later
   - **Priority**: üü¢ LOW - Would be nice for learning

2. **Example Documents/Queries**
   - **Feature**: Pre-load sample PDFs and example queries
   - **Priority**: üü¢ LOW - Improves onboarding

3. **Streaming Responses**
   - **Feature**: Show incremental LLM output
   - **Priority**: üü¢ LOW - Better UX but not critical

4. **Singleton Service Clients**
   - **Feature**: Share clients across sessions
   - **Priority**: üü¢ LOW - Minor optimization

5. **Optional SQLite Persistence**
   - **Feature**: Persist conversations between restarts
   - **Priority**: üü¢ LOW - Current in-memory is fine

#### ‚ö™ Issues NOT Applicable (Production-Only Concerns)

**SKIP - Not relevant for local dev:**

- ‚ö™ Async/await refactor (unnecessary for single user)
- ‚ö™ Complex connection pooling (localhost doesn't need it)
- ‚ö™ Authentication system (local-only by design)
- ‚ö™ Horizontal scaling (single machine)
- ‚ö™ Load balancing (single user)
- ‚ö™ Advanced monitoring (logs are sufficient)
- ‚ö™ CI/CD pipeline (optional for personal projects)

### Action Items by Phase

#### Phase 5: Testing & Documentation (Current)

**Must Add Based on Review:**

- [ ] **Step 19.5: UX Improvements** (2-3 days)
  - [ ] Add progress spinners to all long operations
  - [ ] Surface errors to UI with actionable messages
  - [ ] Complete Knowledge Base upload/search features
  - [ ] Improve startup error messages with retry logic
  - [ ] Add "LOCAL DEV ONLY" warnings to README and docker-compose

- [ ] **Step 19.6: Example Content** (1 day)
  - [ ] Add sample PDF documents
  - [ ] Add example queries in Chat tab
  - [ ] Add quick start tutorial in UI

- [ ] **Step 19.7: Documentation Updates** (1 day)
  - [ ] Clarify "local development playground" in README
  - [ ] Add warning about not exposing to internet
  - [ ] Document default passwords as localhost-only
  - [ ] Add troubleshooting guide

- [ ] **Step 19.8: Testing Fixes** (1-2 days)
  - [ ] Enable skipped security tests
  - [ ] Add basic integration test (e2e workflow)
  - [ ] Test error scenarios properly

#### Phase 6: Polish & Hardening (Future)

**Optional Improvements:**

- [ ] Export/import conversations
- [ ] Streaming responses in chat
- [ ] Better RAG pipeline visualization
- [ ] Conversation history viewer
- [ ] Performance profiling

### Review Summary

**Grade**: B+ (85/100) - Well-designed for local experimentation

**Breakdown**:
- Setup & Getting Started: A (90/100)
- Architecture for Purpose: B+ (88/100)
- Code Quality: B (83/100)
- User Experience: B- (78/100) ‚Üê **Main area for improvement**
- Documentation: B- (80/100)
- Testing: C+ (75/100)
- Feature Completeness: B- (78/100)

**Recommendation**: This is **ready for its intended use** as a local playground, with targeted UX improvements in Phase 5 that would significantly enhance the learning experience.

---

## Progress Tracking

### Quick Status Reference

```
Phase 1: Foundation & Infrastructure
‚îú‚îÄ Step 1: Project Structure ............................ ‚úÖ COMPLETED
‚îú‚îÄ Step 2: Docker Compose ............................... ‚úÖ COMPLETED
‚îú‚îÄ Step 3: DevContainer ................................. ‚úÖ COMPLETED
‚îú‚îÄ Step 4: Configuration Management ..................... ‚úÖ COMPLETED
‚îî‚îÄ Step 5: Repository Validation ........................ ‚úÖ COMPLETED

Phase 2: Core Application Skeleton
‚îú‚îÄ Step 6: Streamlit UI .................................. ‚úÖ COMPLETED
‚îú‚îÄ Step 7: Service Connectivity .......................... ‚úÖ COMPLETED (156 tests)
‚îú‚îÄ Step 8: Health Check & Startup ........................ ‚úÖ COMPLETED (20 tests)
‚îî‚îÄ Phase 2 Total: 196 unit tests, 1,740 LOC

Phase 3: RAG Pipeline Integration
‚îú‚îÄ Step 9: Document Ingestion ............................ ‚úÖ COMPLETED (20 tests)
‚îú‚îÄ Step 10: Retrieval Engine ............................. ‚úÖ COMPLETED (18 tests)
‚îú‚îÄ Step 11: Agent Orchestration .......................... ‚úÖ COMPLETED (22 tests)
‚îú‚îÄ Step 12: Conversation Memory .......................... ‚úÖ COMPLETED (30 tests)
‚îî‚îÄ Phase 3 Total: 92 unit tests, 1,550+ LOC

Phase 4: Security & Observability
‚îú‚îÄ Step 13: LLM Guard Integration ........................ ‚úÖ COMPLETED (24 tests)
‚îú‚îÄ Step 14: Langfuse Observability ....................... ‚úÖ COMPLETED (22 tests)
‚îî‚îÄ Step 15: Environment Configuration ................... ‚úÖ COMPLETED (38 tests)
‚îî‚îÄ Phase 4 Total: 84 unit tests, 850+ LOC

Phase 4b: Tool Dashboards & Management Interface ‚≠ê NEW
‚îú‚îÄ Dashboard Design Document ............................. ‚úÖ COMPLETE (see DASHBOARD_DESIGN.md)
‚îú‚îÄ Step 17: Sidebar Navigation Refactor .................. ‚úÖ COMPLETED (13 tests)
‚îú‚îÄ Step 18: Qdrant Manager Dashboard ..................... ‚úÖ COMPLETED (23 tests)
‚îú‚îÄ Step 19: Langfuse Observability Dashboard ............ ‚úÖ COMPLETED (24 tests)
‚îú‚îÄ Step 20: Promptfoo Testing Dashboard ................. ‚è≠Ô∏è Skipped (Optional)
‚îú‚îÄ Step 21: System Health Dashboard ..................... ‚úÖ COMPLETED (15 tests)
‚îú‚îÄ Step 22: Integration & Validation ..................... ‚úÖ COMPLETED (23 tests)
‚îî‚îÄ Phase 4b Total: 98+ tests completed, 5/6 steps complete (83%) ‚≠ê PHASE NEARLY COMPLETE

Phase 5: Testing, Documentation & UX Polish ‚≠ê COMPLETED
‚îú‚îÄ Step 16: Pytest Setup ................................. ‚úÖ COMPLETED (conftest.py enhanced)
‚îú‚îÄ Step 17: Unit Tests ................................... ‚úÖ COMPLETED (404 tests passing)
‚îú‚îÄ Step 18: Integration Tests ............................ ‚úÖ COMPLETED (23 integration tests)
‚îú‚îÄ Step 19: Documentation ................................ ‚è≥ Not Started
‚îú‚îÄ Step 19.5: UX Improvements (Code Review) .............. ‚úÖ COMPLETED (progress indicators, error visibility)
‚îú‚îÄ Step 19.6: Example Content ............................ ‚úÖ COMPLETED (sample docs + example queries)
‚îú‚îÄ Step 19.7: Documentation Polish (Code Review) ......... ‚úÖ COMPLETED (README + Docker security warnings)
‚îî‚îÄ Step 19.8: Testing Improvements (Code Review) ......... ‚úÖ COMPLETED (0 skipped, 404 passing)
```

### Update Log

| Date       | Phase | Step   | Status       | Notes                                                        |
| ---------- | ----- | ------ | ------------ | ------------------------------------------------------------ |
| 2026-01-10 | -     | Plan   | ‚úÖ Approved  | Plan reviewed and approved                                   |
| 2026-01-10 | 1     | 1      | ‚úÖ Completed | Project structure and configuration initialized              |
| 2026-01-11 | 1     | 2      | ‚úÖ Completed | Docker Compose with 6 services and resource limits           |
| 2026-01-15 | 1     | 3      | ‚úÖ Completed | DevContainer with VS Code integration and tooling            |
| 2026-01-15 | 1     | 4      | ‚úÖ Completed | Configuration management with Pydantic v2 and logging        |
| 2026-01-15 | 1     | 5      | ‚úÖ Completed | Repository validation and structure complete                 |
| 2026-01-16 | 2     | 6-8    | ‚úÖ Completed | Streamlit UI, service clients, startup (196 tests)           |
| 2026-01-17 | 3     | 9-12   | ‚úÖ Completed | RAG pipeline: ingestion, retrieval, agent, memory (92 tests) |
| 2026-01-18 | 4     | 13     | ‚úÖ Completed | LLM Guard integration with 24 unit tests                     |
| 2026-01-15 | 1     | 5      | ‚úÖ Completed | Repository structure validation complete                     |
| 2026-01-15 | 1     | -      | ‚úÖ PHASE 1   | All infrastructure foundation ready for Phase 2              |
| 2026-01-18 | 3     | 9      | ‚úÖ Completed | Document ingestion pipeline with PDF extraction (20 tests)   |
| 2026-01-18 | 3     | 10     | ‚úÖ Completed | Hybrid retrieval engine (semantic + keyword) (18 tests)      |
| 2026-01-18 | 3     | 11     | ‚úÖ Completed | Agent orchestration with LangChain integration (22 tests)    |
| 2026-01-18 | 3     | 12     | ‚úÖ Completed | Multi-turn conversation memory management (30 tests)         |
| 2026-01-18 | 3     | -      | ‚úÖ PHASE 3   | RAG pipeline complete with 92 tests, ready for Phase 4       |
| 2026-01-18 | 4b    | Design | ‚úÖ Completed | Dashboard design document created (DASHBOARD_DESIGN.md)      |
| 2026-01-29 | 4b    | 19     | ‚úÖ Completed | Langfuse Observability Dashboard (24 tests)                  |
| 2026-01-29 | 4b    | 21     | ‚úÖ Completed | System Health Dashboard (15 tests)                           |
| 2026-01-29 | 4b    | 22     | ‚úÖ Completed | Integration Testing & Validation (23 tests)                  |
| 2026-01-29 | 4b    | -      | ‚úÖ PHASE 4b  | Dashboard tools complete - 98+ tests, 399 total passing      |
| 2026-01-29 | 5     | 19.6   | ‚úÖ Completed | Sample documents + example queries in Chat UI                |
| 2026-01-29 | 5     | 19.7   | ‚úÖ Completed | README security warnings + Docker Compose comments           |
| 2026-01-29 | 5     | 19.8   | ‚úÖ Completed | Fixed 5 skipped security tests, 404 total passing            |
| 2026-01-30 | 5     | 19.5   | ‚úÖ Completed | UX improvements: progress indicators, error visibility       |

---

### PHASE 6: Intelligent Retrieval & Web Search

**Goal**: Optimize agent performance with smart retrieval routing and add web search capabilities for external information access.

**Performance Impact**:
- Current: Every query triggers embedding generation (~4-5s)
- Target: Smart routing reduces unnecessary retrievals by 60-70%
- General queries: ~2-3s (no retrieval)
- KB queries: ~5-6s (with retrieval)
- Web-enhanced queries: ~6-9s (with web fetch)

---

#### Step 23: Intelligent Retrieval Optimization

**Status**: üîÑ TODO

**Objective**: Implement three-stage cascading filter to prevent unnecessary document retrieval, reducing latency for general knowledge queries.

**Implementation**:

- [ ] **Stage 1: Empty Knowledge Base Check** (`src/core/agent.py`):
  - [ ] Add `_check_kb_availability()` method
  - [ ] Query Qdrant for document count before retrieval
  - [ ] If `document_count == 0`: Skip retrieval, log warning, proceed to LLM
  - [ ] Cache result for 60 seconds (KB doesn't change often)
  - [ ] **Cost**: 0ms (instant exit when KB empty)

- [ ] **Stage 2: Intent Classification** (`src/core/agent.py`):
  - [ ] Add `_classify_query_intent(user_message: str) -> bool` method
  - [ ] Fast LLM prompt (temperature=0.1):
    ```
    Classify if this query needs document retrieval from a knowledge base.
    Return only "YES" or "NO".
    
    Examples:
    - "What is AI?" ‚Üí NO (general knowledge)
    - "Explain the RAG architecture from the docs" ‚Üí YES (specific to KB)
    - "What time is it?" ‚Üí NO (tool/factual)
    - "Summarize the ingestion pipeline" ‚Üí YES (KB-specific)
    
    Query: {user_message}
    Classification:
    ```
  - [ ] Parse response: `needs_kb = "YES" in response.upper()`
  - [ ] Log decision with reasoning for observability
  - [ ] **Cost**: 1-2s (fast single-token LLM call)
  - [ ] **Benefit**: Avoids 4-5s embedding generation for general queries

- [ ] **Stage 3: Semantic Similarity Threshold** (`src/core/retrieval.py`):
  - [ ] Add `min_relevance_score` parameter to `retrieve_relevant_docs()`
  - [ ] Default threshold: `0.7` (configurable via env var)
  - [ ] After retrieval, filter results: `results = [r for r in results if r.score >= threshold]`
  - [ ] If all results filtered out: Log "Low relevance", return empty list
  - [ ] Agent handles empty list gracefully: "No relevant documents found"
  - [ ] **Cost**: 0ms (applies to existing retrieval results)

- [ ] **Configuration** (`src/config.py`):
  - [ ] Add `retrieval` section to config:
    ```python
    class RetrievalConfig(BaseSettings):
        enable_intent_classification: bool = True
        intent_classification_temperature: float = 0.1
        min_similarity_threshold: float = 0.7
        kb_check_cache_ttl: int = 60  # seconds
    ```
  - [ ] Environment variables:
    - `APP_RETRIEVAL__ENABLE_INTENT_CLASSIFICATION=true`
    - `APP_RETRIEVAL__MIN_SIMILARITY_THRESHOLD=0.7`

- [ ] **Timing Instrumentation**:
  - [ ] Add timing logs for each stage:
    - `[TIMING] KB availability check: 0.00s (empty)`
    - `[TIMING] Intent classification: 1.23s (needs_kb=False)`
    - `[TIMING] Similarity filtering: 0.00s (2/5 results above 0.7)`
  - [ ] Track in Langfuse: `retrieval_skipped`, `intent_classification_result`

- [ ] **Process Flow**:
  ```python
  # Stage 1: Empty KB check (0ms)
  if not _check_kb_availability():
      logger.info("KB empty, skipping retrieval")
      use_retrieval = False
  
  # Stage 2: Intent classification (1-2s)
  elif not _classify_query_intent(user_message):
      logger.info("General query, no KB context needed")
      use_retrieval = False
  
  # Stage 3: Proceed with retrieval (4-5s)
  else:
      docs = retrieval_engine.retrieve_relevant_docs(
          query=user_message,
          top_k=5,
          min_score=0.7  # Stage 3: Similarity threshold
      )
      if not docs:
          logger.warning("No documents above similarity threshold")
  ```

- [ ] **Create Test Suite** (`tests/core/test_agent_retrieval.py`):
  - [ ] Test empty KB bypass (2 tests)
  - [ ] Test intent classification with various query types (8 tests)
    - [ ] General knowledge queries ‚Üí no retrieval
    - [ ] KB-specific queries ‚Üí retrieval
    - [ ] Edge cases: ambiguous queries, multi-intent
  - [ ] Test similarity threshold filtering (5 tests)
    - [ ] All results above threshold ‚Üí return all
    - [ ] Some below threshold ‚Üí filter out
    - [ ] All below threshold ‚Üí return empty
  - [ ] Test cascading logic integration (6 tests)
  - [ ] Test performance impact (mock timing comparisons)
  - [ ] **Minimum: 21 unit tests**

**Deliverable**: Smart retrieval routing that skips unnecessary KB lookups

**Success Criteria**:
- ‚úì General queries ("What is AI?") complete in 2-3s (no retrieval)
- ‚úì KB queries with empty KB complete in 2-3s (no embedding generation)
- ‚úì KB queries with low similarity complete in 5-6s (retrieve but ignore)
- ‚úì Relevant KB queries use retrieved documents (normal flow)
- ‚úì 60-70% reduction in unnecessary retrieval operations
- ‚úì 21+ tests passing
- ‚úì Timing logs show each stage
- ‚úì Langfuse tracks intent classification decisions

**Performance Metrics**:
| Query Type | Before | After | Savings |
|------------|--------|-------|---------|
| Empty KB | 7-8s | 2-3s | -5s (70%) |
| General knowledge | 7-8s | 3-4s | -4s (50%) |
| KB-relevant | 7-8s | 7-8s | 0s |
| Low relevance | 7-8s | 7-8s | 0s (but aware) |

---

#### Step 24: Web Search Tool Integration

**Status**: üîÑ TODO

**Objective**: Enable agent to fetch and parse external web content when KB documents don't contain needed information.

**Implementation**:

- [ ] **Add Dependencies** (`pyproject.toml`):
  ```toml
  dependencies = [
      "httpx>=0.27.0",        # Modern async HTTP client
      "beautifulsoup4>=4.12.0",  # HTML parsing
      "html2text>=2024.2.0",     # HTML to markdown conversion
      "markdownify>=0.13.0",     # Alternative HTML converter
      "urllib3>=2.0.0",          # URL parsing and validation
  ]
  ```

- [ ] **Create Web Search Service** (`src/services/web_search_client.py`):
  - [ ] Class: `WebSearchClient`:
    - [ ] `__init__(timeout: int = 10, max_content_size: int = 500_000)`
    - [ ] HTTP client with retry strategy (3 retries, exponential backoff)
    - [ ] User-Agent header: `"Mozilla/5.0 (Agent-Zero/1.0)"`
    - [ ] Respect `robots.txt` (check before fetching)
  
  - [ ] Method: `fetch_url(url: str) -> WebSearchResult`:
    - [ ] **URL Validation**:
      - [ ] Schema must be `http://` or `https://`
      - [ ] No localhost, 127.0.0.1, or private IP ranges (security)
      - [ ] Valid domain format
      - [ ] Reject file://, ftp://, data:// schemes
    - [ ] **Request Configuration**:
      - [ ] Timeout: 10 seconds (configurable)
      - [ ] Max redirects: 3
      - [ ] SSL verification: enabled
      - [ ] Max content size: 500KB (prevent memory issues)
      - [ ] Stream response and check size before full download
    - [ ] **Rate Limiting**:
      - [ ] In-memory cache: 1 req/second per domain
      - [ ] Simple token bucket algorithm
      - [ ] Log rate limit hits
    - [ ] **Error Handling**:
      - [ ] Timeout ‚Üí return error with timeout message
      - [ ] HTTP 4xx/5xx ‚Üí return error with status code
      - [ ] Network error ‚Üí return error with connection details
      - [ ] SSL error ‚Üí return error about certificate
      - [ ] All errors logged to Langfuse

  - [ ] Method: `parse_html(html: str) -> str`:
    - [ ] Remove: `<script>`, `<style>`, `<nav>`, `<footer>`, `<aside>`, `<iframe>`
    - [ ] Remove: ads, tracking pixels, social media embeds
    - [ ] Remove: comments, forms (unless relevant)
    - [ ] Convert to clean markdown using `html2text`:
      - [ ] Preserve headings hierarchy
      - [ ] Preserve links with text
      - [ ] Preserve code blocks
      - [ ] Remove excessive whitespace
    - [ ] Truncate to max 10,000 characters for LLM context
    - [ ] Return structured content: title, main content, meta description

  - [ ] Method: `extract_main_content(soup: BeautifulSoup) -> str`:
    - [ ] Heuristic content extraction:
      - [ ] Look for `<article>`, `<main>` tags first
      - [ ] Fall back to largest `<div>` with text
      - [ ] Use readability score (text/HTML ratio)
    - [ ] Remove boilerplate (headers, footers, sidebars)

- [ ] **Create Data Model** (`src/models/web_search.py`):
  ```python
  @dataclass
  class WebSearchResult:
      url: str
      title: str
      content: str  # Cleaned markdown
      metadata: Dict[str, Any]  # meta tags, author, date
      success: bool
      error: Optional[str] = None
      fetch_time: float = 0.0  # seconds
      content_size: int = 0  # bytes
  ```

- [ ] **Integrate as Agent Tool** (`src/core/agent.py`):
  - [ ] Add to `self.tools`:
    ```python
    "web_search": self._web_search
    ```
  - [ ] Method: `_web_search(url: str) -> Dict[str, Any]`:
    - [ ] Call `WebSearchClient.fetch_url(url)`
    - [ ] Return cleaned content or error
    - [ ] Log to Langfuse: tool use, URL, success/failure
    - [ ] Track timing: fetch + parse
  
  - [ ] LLM System Prompt Enhancement:
    ```
    Available tools:
    - retrieve_documents: Search local knowledge base
    - web_search(url): Fetch and read web page content
      * Use when KB doesn't have needed info
      * Provide full URL (must start with http:// or https://)
      * Returns cleaned text content from the page
    
    Decision flow:
    1. Check if KB might have info ‚Üí retrieve_documents
    2. If KB empty/irrelevant ‚Üí consider web_search
    3. Always cite sources (KB or web URL)
    ```

- [ ] **Configuration** (`src/config.py`):
  ```python
  class WebSearchConfig(BaseSettings):
      enabled: bool = True
      timeout: int = 10  # seconds
      max_content_size: int = 500_000  # bytes
      max_retries: int = 3
      rate_limit_per_domain: float = 1.0  # requests/second
      user_agent: str = "Mozilla/5.0 (Agent-Zero/1.0)"
      respect_robots_txt: bool = True
  ```
  - [ ] Environment variables:
    - `APP_WEB_SEARCH__ENABLED=true`
    - `APP_WEB_SEARCH__TIMEOUT=10`

- [ ] **Security & Safety**:
  - [ ] URL whitelist/blacklist support (optional)
  - [ ] Content-Type validation (only HTML/text)
  - [ ] Malware check headers (if available)
  - [ ] Log all fetched URLs for audit
  - [ ] Respect website terms of service

- [ ] **Create Test Suite** (`tests/services/test_web_search_client.py`):
  - [ ] Test URL validation (6 tests):
    - [ ] Valid URLs accepted
    - [ ] Invalid schemas rejected (file://, localhost)
    - [ ] Private IPs rejected
    - [ ] Malformed URLs rejected
  - [ ] Test HTTP fetching (8 tests):
    - [ ] Successful fetch returns content
    - [ ] Timeout handling
    - [ ] HTTP error codes (404, 500)
    - [ ] Network errors
    - [ ] Large content size rejection
    - [ ] SSL errors
  - [ ] Test HTML parsing (6 tests):
    - [ ] Script/style removal
    - [ ] Markdown conversion
    - [ ] Main content extraction
    - [ ] Truncation at max length
  - [ ] Test rate limiting (4 tests):
    - [ ] Multiple requests to same domain throttled
    - [ ] Different domains not affected
  - [ ] Test agent tool integration (5 tests):
    - [ ] Tool called correctly
    - [ ] Results returned to agent
    - [ ] Errors handled gracefully
  - [ ] Mock all HTTP requests (no real web calls)
  - [ ] **Minimum: 29 unit tests**

- [ ] **UI Enhancement** (`src/ui/tools/chat.py`):
  - [ ] Show web fetch indicator: "üåê Fetching URL..."
  - [ ] Display fetched URL in sources
  - [ ] Show fetch time in message metadata

**Deliverable**: Agent can fetch and parse web pages for external information

**Success Criteria**:
- ‚úì Agent successfully fetches and parses HTML to markdown
- ‚úì URL validation prevents malicious/invalid URLs
- ‚úì Rate limiting prevents abuse (1 req/s per domain)
- ‚úì Timeouts prevent hanging (10s limit)
- ‚úì Large content rejected (>500KB)
- ‚úì Cleaned content preserves structure (headings, links)
- ‚úì Web search tracked in Langfuse
- ‚úì 29+ tests passing
- ‚úì No real HTTP calls in tests (all mocked)

**Example Usage**:
```
User: "What's the latest Python release?"
Agent: "I don't have that in the KB. Let me check python.org..."
‚Üí Calls web_search("https://www.python.org/downloads/")
‚Üí Parses HTML, extracts version info
‚Üí Returns: "According to python.org, the latest stable release is..."
```

---

### PHASE 6 Validation Checkpoint

**Phase 6 Complete When**:

- [ ] General queries ("What is 2+2?") skip retrieval and answer in 2-3s
- [ ] KB queries with empty KB skip embedding generation
- [ ] Intent classification correctly routes 90%+ of queries
- [ ] Similarity threshold filters low-relevance results
- [ ] Agent can fetch web pages when prompted
- [ ] Web content is cleaned and readable
- [ ] Rate limiting prevents abuse
- [ ] All security validations working (URL, size, timeout)
- [ ] 50+ new tests passing (21 retrieval + 29 web search)
- [ ] Performance improvement visible in Langfuse traces

**Performance Target**:
- 60-70% of queries complete faster (2-4s vs 7-8s)
- Web-enhanced queries complete in 6-9s (acceptable for external fetch)
- No regression for KB-relevant queries

---

## Next Steps

1. **Phase 6**: Implement intelligent retrieval + web search
2. **Documentation**: Create ARCHITECTURE.md and CONTRIBUTING.md  
3. **Production**: Deploy with all security and monitoring features enabled

---

**Document Version**: 1.7  
**Last Updated**: 2026-02-02  
**Maintained By**: Senior AI Architect  
**Status**: Phase 5 Complete - Phase 6 planned (intelligent retrieval + web search)  
**Code Review**: Completed 2026-01-28 (see CODE_REVIEW_ADDENDUM.md)
**Test Suite**: 404 tests passing, 0 skipped ‚Üí Target: 454+ after Phase 6
